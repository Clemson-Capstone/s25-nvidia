{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b998a5c-45a6-4f05-9c7c-b0f3c4c74c17",
   "metadata": {},
   "source": [
    "# Virtual Teaching Assistant NVIDIA AI Blueprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f2acb-52d9-49dd-9676-2d58715786f5",
   "metadata": {},
   "source": [
    "Imagine a teaching assistant that never sleeps, never gets tired, and is always ready to help. Our Virtual Teaching Assistant (VTA) makes that possible, transforming static educational content into dynamic learning experiences. Built for modern classrooms and self-learners alike, VTA is designed to:\n",
    "\n",
    "- Break down complex course materials into clear, conversational explanations\n",
    "\n",
    "- Guide students through material and help them arrive at answers on their own\n",
    "\n",
    "- Adapt its tone and depth based on user needs ‚Äî from quick summaries to in-depth walkthroughs\n",
    "\n",
    "- Utilize powerful language models via NVIDIA NIM to understand and explain academic content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dd4f69",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"nvidiaragimg.png\" alt=\"NVIDIA RAG Diagram\" style=\"width:80%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af66273-f19d-4d4f-8480-31c5c03c5951",
   "metadata": {},
   "source": [
    "## Features\n",
    "Built on the NVIDIA RAG Blueprint, this system is optimized for fast setup, multimodal data handling, and private, on-prem inference or cloud deployment. Launch the full stack with a single make all. **We've built this for you to edit and deploy on your own infrastructure with ease.**\n",
    "\n",
    "#### Core Capabilities\n",
    "- **Canvas Integration  üìö** - The Course Manager API provides a RESTful interface to authenticate with Canvas, retrieve course data, and download materials. It integrates with the RAG server to enable AI-powered content processing.\n",
    "- **Guardrailed Conversations üõ°Ô∏è** - Uses NeMo Guardrails to maintain safe, educational, and on-topic assistant responses‚Äîideal for a classroom or learning environment.\n",
    "- **Multimodal Ingestion üìÑ** - Extracts text, tables, charts, and images from PDFs, DOCX, and PPTX files using GPU-accelerated NIM services.\n",
    "- **On-Prem LLM & Retrieval üß†** - Locally hosts embedding, reranking, and inference microservices using Meta Llama and NVIDIA models‚Äîensuring low latency and data privacy.\n",
    "- **Hybrid Semantic Search üåê** - Combines dense and sparse search for accurate academic content retrieval with support for multilingual queries.\n",
    "- **Context-Aware Responses üî•** - Supports multi-turn Q&A with reranking and query rewriting for enhanced dialogue quality.\n",
    "\n",
    "\n",
    "#### Development Experience\n",
    "- **One-Command Deployment ‚öôÔ∏è** - Launch ingestion, RAG, NIM services, and the playground UI with a single make all.\n",
    "- **Docker Compose Integration üê≥** - one command (`make all`) spins up the entire stack, with smart handling of GPU resources and service dependencies.\n",
    "\n",
    "\n",
    "#### Extend and Customize (More Information in nvidia-rag-2.0/docs)\n",
    "- **Swap Inference or Embedding Models üîÅ** - Easily change the LLM or embedding model to match your performance or domain needs.\n",
    "- **Customize Prompts and Parameters üéõÔ∏è** - Tailor prompt templates and LLM parameters at runtime for better control.\n",
    "- **Turn on Image Captioning üñºÔ∏è** - Add vision-language model support to describe visual content.\n",
    "- **Activate Hybrid Search üîç** - Combine sparse and dense retrieval to improve chunk relevance.\n",
    "- **Optimize for Text-Only Mode ‚ö°** - Reduce latency and compute for lightweight use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fcc47-fb41-4e54-9d30-4d17bc483779",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09678c9-8fbe-41d7-84ad-ce624bec582c",
   "metadata": {},
   "source": [
    "### Clone the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a197704e-b63c-42fc-be4b-4f3fb03acfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 's25-nvidia'...\n",
      "remote: Enumerating objects: 1419, done.\u001b[K\n",
      "remote: Counting objects: 100% (370/370), done.\u001b[K\n",
      "remote: Compressing objects: 100% (217/217), done.\u001b[K\n",
      "remote: Total 1419 (delta 145), reused 302 (delta 100), pack-reused 1049 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1419/1419), 2.57 MiB | 4.85 MiB/s, done.\n",
      "Resolving deltas: 100% (497/497), done.\n"
     ]
    }
   ],
   "source": [
    "# Note this is my personal token url\n",
    "!git clone "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec102b-21d3-441c-a1a8-e72dbfc6c6fd",
   "metadata": {},
   "source": [
    "## Get an NVIDIA NIM Trial API Key\n",
    "\n",
    "Prior to getting started, you will need to create API Keys to access NVIDIA NIM trial hosted endpoints.\n",
    "\n",
    "If you don‚Äôt have an NVIDIA account, you will be asked to sign-up.\n",
    "\n",
    "Click [here](https://build.nvidia.com/meta/llama-3_3-70b-instruct?signin=true&api_key=true) to sign-in and get an API key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d68974-f114-4763-badb-9a158582f2e3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Tip:</b> The key begins with the letters nvapi-."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc204aa9-8a1b-4cfe-82fd-ac35a0362407",
   "metadata": {},
   "source": [
    "## Set Environment Variables\n",
    "\n",
    "This notebook requires certain environment variables to be configured. We'll help you set these up in a `.env` file.\n",
    "\n",
    "Required variables:\n",
    "- `NVIDIA_API_KEY`: Your NVIDIA API key\n",
    "- `MAX_CONCURRENT_REQUESTS`: Number of concurrent requests allowed (recommended: 1 for local development)\n",
    "\n",
    "Run the code cell below to create your `.env` file. Make sure to replace the placeholder values with your actual API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9aba6-f362-40fb-a0a7-eaf9e6641886",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd s25-nvidia/\n",
    "    \n",
    "# Backup existing .env if it exists\n",
    "if [ -f .env ]; then\n",
    "    echo \"Warning: .env file already exists. Backing up to .env.backup\"\n",
    "    mv .env .env.backup\n",
    "fi\n",
    "\n",
    "# Create new .env file\n",
    "cat > .env << EOL\n",
    "NVIDIA_API_KEY=<ENTER_KEY>\n",
    "MAX_CONCURRENT_REQUESTS=1\n",
    "EOL\n",
    "\n",
    "echo \"Created .env file. Please edit it with your actual API keys.\"\n",
    "echo -e \"\\nCurrent .env contents:\"\n",
    "echo \"----------------------------------------\"\n",
    "cat .env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1317ada1-3f8d-4e98-b07d-91cf2975498b",
   "metadata": {},
   "source": [
    "## Install Dependancies\n",
    "\n",
    "You can install them by simply running `make setup` in the root of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f1c4f-22f2-40da-9fad-fbaed4db8e9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Clone the repo\n",
    "cd s25-nvidia/\n",
    "\n",
    "# Making setup script executable\n",
    "make setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# In seperate terminal window run\n",
    "cd s25-nvidia/\n",
    "make all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa7f3c4-d8c3-4b96-bb43-5c15c4c4918b",
   "metadata": {},
   "source": [
    "## Spin Up Blueprint\n",
    "Docker compose scripts are provided which spin up the microservices on a single node. This docker-compose yaml file will start up each microservice. This may take up to **15 minutes** to complete.\n",
    "\n",
    "> **In a separate terminal window, run**\n",
    "\n",
    "```\n",
    "cd s25-nvidia/\n",
    "make all\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9118384b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf647f-0f0b-45e2-959b-d96b013169a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps --format \"table {{.ID}}\\t{{.Names}}\\t{{.Status}}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d90c358-f0e9-4607-8b88-32a44ffce74e",
   "metadata": {},
   "source": [
    "This command should produce similiar output in the following format:\n",
    "\n",
    "```\n",
    "NAMES                                   STATUS\n",
    "compose-nv-ingest-ms-runtime-1          Up 5 minutes (healthy)\n",
    "ingestor-server                         Up 5 minutes\n",
    "compose-redis-1                         Up 5 minutes\n",
    "rag-playground                          Up 9 minutes\n",
    "rag-server                              Up 9 minutes\n",
    "milvus-standalone                       Up 36 minutes\n",
    "milvus-minio                            Up 35 minutes (healthy)\n",
    "milvus-etcd                             Up 35 minutes (healthy)\n",
    "nemoretriever-ranking-ms                Up 38 minutes (healthy)\n",
    "compose-page-elements-1                 Up 38 minutes\n",
    "compose-paddle-1                        Up 38 minutes\n",
    "compose-graphic-elements-1              Up 38 minutes\n",
    "compose-table-structure-1               Up 38 minutes\n",
    "nemoretriever-embedding-ms              Up 38 minutes (healthy)\n",
    "nim-llm-ms                              Up 38 minutes (healthy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f0d682-b20b-4dca-b966-db6605d9dadf",
   "metadata": {},
   "source": [
    "You can check if the services are up by running the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45f128-fe7e-4f9d-99bb-b23f8fbc4b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl localhost:8002/health"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c9bc5-6b6b-471a-b122-f029b20b2fdb",
   "metadata": {},
   "source": [
    "Open a web browser and access http://localhost:3000 to use our frontend. You can use the upload tab to ingest files into the server or follow the notebooks to understand the API usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ebd50",
   "metadata": {},
   "source": [
    "Note: If you are running this as a launchable, you can access the the API endpoint, Jaeger UI, and the MinIO Object Storage UI by going to your running launchable on Brev, clicking `Access`, and clicking the links in the `Deployments` section. It should look like the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509931b7",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/brevdev/notebooks/raw/main/assets/ara-launchable/services.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2293d461-8cd1-48fb-bce7-dbf8515ea3f4",
   "metadata": {},
   "source": [
    "## Create a podcast!\n",
    "\n",
    "For this example, we'll directly call the API to generate the podcast. First we write some helper functions to interact with the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e18296-a63f-4745-9d5a-3f0148ffabe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from typing import List\n",
    "from IPython.display import Audio\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_URL = \"http://localhost:8002\"\n",
    "\n",
    "def generate_podcast(\n",
    "    target_pdf_paths: List[str], \n",
    "    name: str,\n",
    "    duration: int,\n",
    "    speaker_1_name: str,\n",
    "    context_pdf_paths: List[str] = None,\n",
    "    is_monologue: bool = False,\n",
    "    speaker_2_name: str = None,\n",
    "    guide: str = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a podcast using the API.\n",
    "    \n",
    "    Args:\n",
    "        target_pdf_paths: List of paths to main PDFs to analyze\n",
    "        name: Name of the podcast\n",
    "        duration: Desired duration in minutes\n",
    "        speaker_1_name: Name of the first speaker\n",
    "        context_pdf_paths: Optional list of paths to context PDFs\n",
    "        is_monologue: Whether to generate a monologue\n",
    "        speaker_2_name: Name of second speaker (required if not monologue)\n",
    "        guide: Optional guidance for the podcast structure\n",
    "    \"\"\"\n",
    "    # Handle single path inputs\n",
    "    if isinstance(target_pdf_paths, str):\n",
    "        target_pdf_paths = [target_pdf_paths]\n",
    "    if isinstance(context_pdf_paths, str):\n",
    "        context_pdf_paths = [context_pdf_paths]\n",
    "    \n",
    "    files = []\n",
    "    \n",
    "    # Add all target PDFs\n",
    "    for pdf_path in target_pdf_paths:\n",
    "        content = Path(pdf_path).read_bytes()\n",
    "        files.append(('target_files', (Path(pdf_path).name, content, 'application/pdf')))\n",
    "    \n",
    "    # Add all context PDFs if provided\n",
    "    if context_pdf_paths:\n",
    "        for pdf_path in context_pdf_paths:\n",
    "            content = Path(pdf_path).read_bytes()\n",
    "            files.append(('context_files', (Path(pdf_path).name, content, 'application/pdf')))\n",
    "    \n",
    "    # Configure voice mapping\n",
    "    voice_mapping = {\n",
    "        \"speaker-1\": \"iP95p4xoKVk53GoZ742B\" \n",
    "    }\n",
    "    if not is_monologue:\n",
    "        voice_mapping[\"speaker-2\"] = \"9BWtsMINqrJLrRacOk9x\"\n",
    "    \n",
    "    # Create parameters\n",
    "    params = {\n",
    "        \"userId\": \"test-userid\",\n",
    "        \"name\": name,\n",
    "        \"duration\": duration,\n",
    "        \"monologue\": is_monologue,\n",
    "        \"speaker_1_name\": speaker_1_name,\n",
    "        \"voice_mapping\": voice_mapping,\n",
    "        \"guide\": guide,\n",
    "        \"vdb_task\": False\n",
    "    }\n",
    "    if not is_monologue:\n",
    "        params[\"speaker_2_name\"] = speaker_2_name\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{BASE_URL}/process_pdf\", \n",
    "        files=files,\n",
    "        data={'transcription_params': json.dumps(params)}\n",
    "    )\n",
    "    if response.status_code != 202:\n",
    "        raise Exception(f\"Failed to submit podcast generation: {response.text}\")\n",
    "    \n",
    "    return response.json()['job_id']\n",
    "\n",
    "def get_status(job_id: str) -> dict:\n",
    "    \"\"\"Get the current status of all services for a job.\"\"\"\n",
    "    response = requests.get(f\"{BASE_URL}/status/{job_id}?userId=test-userid\")\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to get status: {response.text}\")\n",
    "    return response.json()\n",
    "\n",
    "def wait_for_completion(job_id: str, check_interval: int = 5, initial_delay: int = 10):\n",
    "    \"\"\"\n",
    "    Poll the status endpoint until the podcast is ready.\n",
    "    Shows a simplified progress view.\n",
    "    \"\"\"\n",
    "    print(f\"Waiting {initial_delay} seconds for job to initialize...\")\n",
    "    time.sleep(initial_delay)\n",
    "    \n",
    "    last_messages = {}  # Track last message for each service to avoid duplication\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            statuses = get_status(job_id)\n",
    "            \n",
    "            # Check each service and only print if status changed\n",
    "            for service, status in statuses.items():\n",
    "                current_msg = status.get('message', '')\n",
    "                if current_msg != last_messages.get(service):\n",
    "                    print(f\"[{service.upper()}] {current_msg}\")\n",
    "                    last_messages[service] = current_msg\n",
    "            \n",
    "            # Check if everything is completed\n",
    "            all_completed = all(\n",
    "                status.get('status') == 'JobStatus.COMPLETED' \n",
    "                for status in statuses.values()\n",
    "            )\n",
    "            \n",
    "            if all_completed and 'tts' in statuses:\n",
    "                print(\"\\nPodcast generation completed!\")\n",
    "                return\n",
    "            \n",
    "            # Check for failures\n",
    "            for service, status in statuses.items():\n",
    "                if status.get('status') == 'JobStatus.FAILED':\n",
    "                    raise Exception(f\"Service {service} failed: {status.get('message')}\")\n",
    "            \n",
    "            time.sleep(check_interval)\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if \"Job not found\" in str(e):\n",
    "                print(\"Waiting for job to start...\")\n",
    "                time.sleep(check_interval)\n",
    "                continue\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddfee58-4bd2-40ad-a2aa-0c10d14a3e23",
   "metadata": {},
   "source": [
    "Next you will generate a monologue using various analyst reports on NVIDIAs most recent financial earnings. You can also add your own PDFs to this Jupyter Lab and point to them in code below. Note that context PDFs are optional and can be used to provide additional context for the generation process. Additonally, you can provide a `guide` to help guide the generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3a2b6-8930-412d-8a6f-678fad2de97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Submitting podcast generation request...\")\n",
    "    job_id = generate_podcast(\n",
    "        target_pdf_paths=[\n",
    "            \"pdf-to-podcast/samples/investorpres-main.pdf\",\n",
    "        ],\n",
    "        context_pdf_paths=[\n",
    "            \"pdf-to-podcast/samples/bofa-context.pdf\",\n",
    "            \"pdf-to-podcast/samples/citi-context.pdf\"\n",
    "        ],\n",
    "        name=\"NVIDIA Earnings Analysis\",\n",
    "        duration=15,\n",
    "        speaker_1_name=\"Alex\",\n",
    "        is_monologue=True,\n",
    "        guide=\"Focus on NVIDIA's earnings and the key points driving it's growth\"\n",
    "    )\n",
    "    print(f\"Job ID: {job_id}\")\n",
    "    wait_for_completion(job_id)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a764dfff",
   "metadata": {},
   "source": [
    "You can also generate a 2 person podcast by calling the same function but setting `is_monologue=False` and providing a `speaker_2_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a24fa8-b024-4a79-9214-d129a6b8c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl \"localhost:8002/output/{job_id}?userId=test-userid\" --output temp_audio.mp3\n",
    "Audio(\"temp_audio.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d8f529-09d4-43fc-8667-bfa48fd9d029",
   "metadata": {},
   "source": [
    "## Understanding Your Generated Podcast\n",
    "\n",
    "After generating your podcast, you have access to a couple valuable outputs that provide insights into the generation process and content. Here's what endpoints you can use to explore:\n",
    "\n",
    "#### 1. The Transcript\n",
    "```python\n",
    "/saved_podcast/{job_id}/transcript\"\n",
    "```\n",
    "\n",
    "The transcript provides a text version of your podcast, which is valuable for:\n",
    "- Quick content review without audio playback\n",
    "- Creating show notes or content summaries\n",
    "- Finding and quoting specific discussion points\n",
    "- Making content searchable and referenceable\n",
    "- Ensuring accessibility of your content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e8f10-6ca5-4143-9883-b479006155b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl \"localhost:8002/saved_podcast/{job_id}/transcript?userId=test-userid\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4180391-05e9-4d5d-a66c-1e6dc6bb3163",
   "metadata": {},
   "source": [
    "#### 2. Prompt and Generation History\n",
    "```python\n",
    "/saved_podcast/{job_id}/history\n",
    "```\n",
    "\n",
    "The history reveals the AI's thought process, showing you:\n",
    "- How the system analyzed and interpreted your PDFs\n",
    "- Key topics and themes identified\n",
    "- The structural decisions made for the conversation\n",
    "- The reasoning behind content organization\n",
    "- How different sections were prioritized and connected\n",
    "\n",
    "This is particularly useful for:\n",
    "- Understanding how the AI makes decisions\n",
    "- Improving future podcast generations\n",
    "- Verifying content accuracy and relevance\n",
    "- Fine-tuning and evals on your prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe01fc0-e111-4fe0-9530-fe549e5892d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!curl \"localhost:8002/saved_podcast/{job_id}/history?userId=test-userid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5642285-2746-47fc-9b47-9f58d33301bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl \"localhost:8002/saved_podcast/{job_id}/metadata?userId=test-userid\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae77e9",
   "metadata": {},
   "source": [
    "### Tools for Understanding Your Generated Podcast\n",
    "\n",
    "After generating your podcast, you can explore the generation process through several tools:\n",
    "\n",
    "#### 1. Jaeger Tracing Interface\n",
    "Access Jaeger at `localhost:16686` to:\n",
    "- Visualize the complete request flow\n",
    "- Debug processing bottlenecks\n",
    "- Monitor service performance\n",
    "- Track PDF processing and audio generation stages\n",
    "\n",
    "#### 2. MinIO Object Storage\n",
    "Access MinIO at `localhost:9001` with:\n",
    "```\n",
    "Username: minioadmin\n",
    "Password: minioadmin\n",
    "```\n",
    "Here you can:\n",
    "- Browse generated audio files\n",
    "- Access intermediate processing artifacts\n",
    "- View stored PDF documents\n",
    "- Download or share content via presigned URLs\n",
    "\n",
    "#### 3. API Endpoints\n",
    "You can access the API endpoint at `localhost:8002/docs`.\n",
    "\n",
    "> **Note**: If you are running this as a Brev launchable, you can access the the API endpoint, Jaeger UI, and the MinIO Object Storage UI by going to your running launchable on Brev, clicking `Access`, and clicking the links in the `Deployments` section. It should look like the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e9c3f1",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/brevdev/notebooks/raw/main/assets/ara-launchable/services.png\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
